{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(dropout, neuronPct, neuronShrink,noisePct):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neuronCount))\n",
    "    model.add(GaussianNoise(noisePct))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(14, activation='sigmoid')) # Output\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "** class_weights **\n",
      "[{0: 0.976060692178489, 1: 0.023939307821511}, {0: 0.9379028967906056, 1: 0.06209710320939444}, {0: 0.977507900874183, 1: 0.02249209912581691}, {0: 0.9801862148908839, 1: 0.01981378510911613}, {0: 0.9642020357560434, 1: 0.03579796424395662}, {0: 0.9663727015263743, 1: 0.033627298473625666}, {0: 0.8859702012473223, 1: 0.11402979875267771}, {0: 0.9586866934982315, 1: 0.04131330650176841}, {0: 0.9623146440112557, 1: 0.03768535598874437}, {0: 0.9298929992036218, 1: 0.07010700079637826}, {0: 0.9335352709009039, 1: 0.06646472909909606}, {0: 0.9021976306069932, 1: 0.09780236939300682}, {0: 0.9453965277787032, 1: 0.05460347222129675}, {1: 0.720226409263611, 0: 0.27977359073638897}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\", class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\", class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 21 **\n"
     ]
    }
   ],
   "source": [
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    "        )\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_network(dropout,lr,neuronPct,neuronShrink,alpha,gamma,noisePct):\n",
    "    # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "    #lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    #lookahead.inject(model) # add into model\n",
    "    output_weights_path = os.path.join(output_dir,  str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+str(noisePct)+\"_\"+output_weights_name)\n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "    model = construct_network(dropout, neuronPct, neuronShrink,noisePct)\n",
    "    \n",
    "    #model.compile(loss=focal_loss(gamma=gamma,alpha=alpha), optimizer=SGD(lr=lr))\n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    model.compile(optimizer=optimizer,loss=focal_loss(gamma=gamma,alpha=alpha))\n",
    "    #lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    #lookahead.inject(model) # add into model\n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+str(noisePct)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    \n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for i in range(len(class_names)):\n",
    "            try:\n",
    "                score = roc_auc_score(y[:, i], y_hat[:, i])\n",
    "                aurocs.append(score)\n",
    "            except ValueError:\n",
    "                score = 0\n",
    "            f.write(f\"{class_names[i]}: {score}\\n\")\n",
    "        mean_auroc = float(np.mean(aurocs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\")\n",
    "    \n",
    "\n",
    "    print(\"iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\")\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    return mean_auroc\n",
    "\n",
    "        \n",
    "    \n",
    "    model.summary()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_bayes_opt.bayesian_optimization import BayesianOptimization\n",
    "import time\n",
    "\n",
    "# Supress NaN warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = { 'gamma':(1.0, 4.0),\n",
    "            'alpha':(0.5, 2.0),\n",
    "            'dropout': (0.05, 0.2),\n",
    "           'lr': (0.009, 0.015),\n",
    "           'neuronPct': (0.1900 , 0.1950),\n",
    "           'neuronShrink': (0.350, 0.360),\n",
    "           'noisePct':(0.1,0.4)\n",
    "          }\n",
    "\n",
    "#print(bounds.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kunci ['alpha', 'dropout', 'gamma', 'lr', 'neuronPct', 'neuronShrink', 'noisePct']\n",
      "Bound  [[0.5   2.   ]\n",
      " [0.05  0.2  ]\n",
      " [1.    4.   ]\n",
      " [0.009 0.015]\n",
      " [0.19  0.195]\n",
      " [0.35  0.36 ]\n",
      " [0.1   0.4  ]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum \n",
    "    # is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "logger = JSONLogger(path=\"./noise12_1.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isine x [1.12553301 0.15804867 1.00034312 0.010814   0.19073378 0.35092339\n",
      " 0.15587806]\n",
      "Konten (1.125533007053861, 0.15804867401632372, 1.0003431244520347, 0.010813995435791039, 0.19073377945408557, 0.35092338594768796, 0.1558780634133013)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.15804867401632372_0.010813995435791039__0.19073377945408557_0.35092338594768796_0.1558780634133013_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 334)               513358    \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 334)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 334)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 334)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4690      \n",
      "=================================================================\n",
      "Total params: 518,048\n",
      "Trainable params: 518,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-9-f954e4d689c5>:45: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -26246.8483\n",
      "Epoch 00001: val_loss improved from inf to -27231.62168, saving model to ./experiments/0.15804867401632372_0.010813995435791039__0.19073377945408557_0.35092338594768796_0.1558780634133013_weights.h5\n",
      "76/76 [==============================] - 23s 304ms/step - loss: -26261.1225 - val_loss: -27231.6217\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -27212.9587\n",
      "Epoch 00002: val_loss did not improve from -27231.62168\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 23s 301ms/step - loss: -27214.5209 - val_loss: -27231.6217\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -27212.9980\n",
      "Epoch 00003: val_loss did not improve from -27231.62168\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 23s 299ms/step - loss: -27214.5597 - val_loss: -27231.6217\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -27213.0132\n",
      "Epoch 00004: val_loss did not improve from -27231.62168\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "76/76 [==============================] - 23s 298ms/step - loss: -27214.5747 - val_loss: -27231.6217\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -27213.0147\n",
      "Epoch 00005: val_loss did not improve from -27231.62168\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "76/76 [==============================] - 23s 298ms/step - loss: -27214.5761 - val_loss: -27231.6217\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -27213.0147\n",
      "Epoch 00006: val_loss did not improve from -27231.62168\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 23s 298ms/step - loss: -27214.5761 - val_loss: -27231.6217\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:From <ipython-input-9-f954e4d689c5>:47: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "21/21 [==============================] - 5s 262ms/step\n",
      "** write log to ./experiments/0.15804867401632372_0.010813995435791039__0.19073377945408557_0.35092338594768796_0.1558780634133013_test.log **\n",
      "mean auroc: 0.5\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (1.125533007053861, 0.15804867401632372, 1.0003431244520347, 0.010813995435791039, 0.19073377945408557, 0.35092338594768796, 0.1558780634133013)\n",
      "Konten (1.125533007053861, 0.15804867401632372, 1.0003431244520347, 0.010813995435791039, 0.19073377945408557, 0.35092338594768796, 0.1558780634133013)\n",
      "Isine x [1.01834109 0.10951512 2.6164502  0.01151517 0.1934261  0.35204452\n",
      " 0.36343523]\n",
      "Konten (1.0183410905645716, 0.1095151211346005, 2.616450202010071, 0.011515167086419769, 0.1934260975019838, 0.35204452249731516, 0.3634352309172837)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.1095151211346005_0.011515167086419769__0.1934260975019838_0.35204452249731516_0.3634352309172837_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 340)               522580    \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 340)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 340)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 340)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4774      \n",
      "=================================================================\n",
      "Total params: 527,354\n",
      "Trainable params: 527,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -3512.8741\n",
      "Epoch 00001: val_loss improved from inf to -3978.69521, saving model to ./experiments/0.1095151211346005_0.011515167086419769__0.1934260975019838_0.35204452249731516_0.3634352309172837_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: -3519.1957 - val_loss: -3978.6952\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -3975.9438\n",
      "Epoch 00002: val_loss did not improve from -3978.69521\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -3976.1724 - val_loss: -3978.6952\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -3975.9575\n",
      "Epoch 00003: val_loss did not improve from -3978.69521\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -3976.1859 - val_loss: -3978.6952\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -3975.9495\n",
      "Epoch 00004: val_loss did not improve from -3978.69521\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -3976.1780 - val_loss: -3978.6952\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -3975.9496\n",
      "Epoch 00005: val_loss did not improve from -3978.69521\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "76/76 [==============================] - 23s 299ms/step - loss: -3976.1781 - val_loss: -3978.6952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -3975.9480\n",
      "Epoch 00006: val_loss did not improve from -3978.69521\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -3976.1765 - val_loss: -3978.6952\n",
      "Epoch 00006: early stopping\n",
      "21/21 [==============================] - 5s 259ms/step\n",
      "** write log to ./experiments/0.1095151211346005_0.011515167086419769__0.1934260975019838_0.35204452249731516_0.3634352309172837_test.log **\n",
      "mean auroc: 0.49996247070602956\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (1.0183410905645716, 0.1095151211346005, 2.616450202010071, 0.011515167086419769, 0.1934260975019838, 0.35204452249731516, 0.3634352309172837)\n",
      "Konten (1.0183410905645716, 0.1095151211346005, 2.616450202010071, 0.011515167086419769, 0.1934260975019838, 0.35204452249731516, 0.3634352309172837)\n",
      "Isine x [0.54108139 0.15057013 2.25191441 0.01235214 0.19070193 0.35198101\n",
      " 0.34022337]\n",
      "Konten (0.5410813897968892, 0.15057012652676036, 2.251914407101381, 0.01235213897067451, 0.19070193469297617, 0.35198101489084876, 0.34022337060266106)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 335)               514895    \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 335)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 335)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 335)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4704      \n",
      "=================================================================\n",
      "Total params: 519,599\n",
      "Trainable params: 519,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 275.9257\n",
      "Epoch 00001: val_loss improved from inf to 259.69538, saving model to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 275.2700 - val_loss: 259.6954\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 237.2675\n",
      "Epoch 00002: val_loss improved from 259.69538 to 254.49243, saving model to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 237.0236 - val_loss: 254.4924\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 231.5269\n",
      "Epoch 00003: val_loss improved from 254.49243 to 251.83704, saving model to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 231.2891 - val_loss: 251.8370\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 228.9830\n",
      "Epoch 00004: val_loss improved from 251.83704 to 250.51462, saving model to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 228.7015 - val_loss: 250.5146\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 226.4078\n",
      "Epoch 00005: val_loss improved from 250.51462 to 249.10950, saving model to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 226.1651 - val_loss: 249.1095\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 225.6946\n",
      "Epoch 00006: val_loss improved from 249.10950 to 248.08602, saving model to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 225.4673 - val_loss: 248.0860\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 224.2938\n",
      "Epoch 00007: val_loss improved from 248.08602 to 247.22693, saving model to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 224.0564 - val_loss: 247.2269\n",
      "21/21 [==============================] - 5s 259ms/step\n",
      "** write log to ./experiments/0.15057012652676036_0.01235213897067451__0.19070193469297617_0.35198101489084876_0.34022337060266106_test.log **\n",
      "mean auroc: 0.8550158358716545\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.5410813897968892, 0.15057012652676036, 2.251914407101381, 0.01235213897067451, 0.19070193469297617, 0.35198101489084876, 0.34022337060266106)\n",
      "Konten (0.5410813897968892, 0.15057012652676036, 2.251914407101381, 0.01235213897067451, 0.19070193469297617, 0.35198101489084876, 0.34022337060266106)\n",
      "Isine x [1.95239236 0.09701363 3.07696785 0.01425833 0.19447303 0.35085044\n",
      " 0.11171643]\n",
      "Konten (1.9523923635790963, 0.09701362672388644, 3.0769678470079422, 0.01425833491377623, 0.19447303331751925, 0.35085044211369776, 0.11171643496986472)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.09701362672388644_0.01425833491377623__0.19447303331751925_0.35085044211369776_0.11171643496986472_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 341)               524117    \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 341)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 341)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 341)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4788      \n",
      "=================================================================\n",
      "Total params: 528,905\n",
      "Trainable params: 528,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -201655.2613\n",
      "Epoch 00001: val_loss improved from inf to -206600.51094, saving model to ./experiments/0.09701362672388644_0.01425833491377623__0.19447303331751925_0.35085044211369776_0.11171643496986472_weights.h5\n",
      "76/76 [==============================] - 23s 301ms/step - loss: -201730.3196 - val_loss: -206600.5109\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -206459.4062\n",
      "Epoch 00002: val_loss did not improve from -206600.51094\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -206471.2521 - val_loss: -206600.5109\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -206459.4062\n",
      "Epoch 00003: val_loss did not improve from -206600.51094\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -206471.2521 - val_loss: -206600.5109\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -206459.4062\n",
      "Epoch 00004: val_loss did not improve from -206600.51094\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -206471.2521 - val_loss: -206600.5109\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -206459.4062\n",
      "Epoch 00005: val_loss did not improve from -206600.51094\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: -206471.2521 - val_loss: -206600.5109\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -206459.4062\n",
      "Epoch 00006: val_loss did not improve from -206600.51094\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 23s 298ms/step - loss: -206471.2521 - val_loss: -206600.5109\n",
      "Epoch 00006: early stopping\n",
      "21/21 [==============================] - 5s 262ms/step\n",
      "** write log to ./experiments/0.09701362672388644_0.01425833491377623__0.19447303331751925_0.35085044211369776_0.11171643496986472_test.log **\n",
      "mean auroc: 0.5\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (1.9523923635790963, 0.09701362672388644, 3.0769678470079422, 0.01425833491377623, 0.19447303331751925, 0.35085044211369776, 0.11171643496986472)\n",
      "Konten (1.9523923635790963, 0.09701362672388644, 3.0769678470079422, 0.01425833491377623, 0.19447303331751925, 0.35085044211369776, 0.11171643496986472)\n",
      "Isine x [0.75474563 0.18172138 1.2950405  0.01152665 0.19478945 0.35533165\n",
      " 0.30756313]\n",
      "Konten (0.7547456293468533, 0.18172137551441198, 1.2950405014991504, 0.011526645750030312, 0.1947894476507525, 0.3553316528497302, 0.30756313418514203)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 345)               530265    \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 345)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 345)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 345)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4844      \n",
      "=================================================================\n",
      "Total params: 535,109\n",
      "Trainable params: 535,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 465.6608\n",
      "Epoch 00001: val_loss improved from inf to 438.36177, saving model to ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 464.6104 - val_loss: 438.3618\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 410.9512\n",
      "Epoch 00002: val_loss improved from 438.36177 to 434.35214, saving model to ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_weights.h5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 410.3756 - val_loss: 434.3521\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 403.4988\n",
      "Epoch 00003: val_loss improved from 434.35214 to 433.46119, saving model to ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_weights.h5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 403.0326 - val_loss: 433.4612\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 400.1331\n",
      "Epoch 00004: val_loss improved from 433.46119 to 433.27120, saving model to ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_weights.h5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 399.7422 - val_loss: 433.2712\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 396.8387\n",
      "Epoch 00005: val_loss improved from 433.27120 to 432.19680, saving model to ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 396.3592 - val_loss: 432.1968\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 396.1288\n",
      "Epoch 00006: val_loss did not improve from 432.19680\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 395.7011 - val_loss: 432.2216\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 392.8881\n",
      "Epoch 00007: val_loss improved from 432.19680 to 431.75914, saving model to ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 392.4063 - val_loss: 431.7591\n",
      "21/21 [==============================] - 5s 261ms/step\n",
      "** write log to ./experiments/0.18172137551441198_0.011526645750030312__0.1947894476507525_0.3553316528497302_0.30756313418514203_test.log **\n",
      "mean auroc: 0.8576525167957695\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.7547456293468533, 0.18172137551441198, 1.2950405014991504, 0.011526645750030312, 0.1947894476507525, 0.3553316528497302, 0.30756313418514203)\n",
      "Konten (0.7547456293468533, 0.18172137551441198, 1.2950405014991504, 0.011526645750030312, 0.1947894476507525, 0.3553316528497302, 0.30756313418514203)\n",
      "Isine x [0.83594565 0.19504139 2.60323013 0.01349044 0.19416043 0.35307686\n",
      " 0.26633987]\n",
      "Konten (0.8359456494426163, 0.19504139478869603, 2.6032301300642007, 0.013490444119825869, 0.19416043034372363, 0.3530768597732787, 0.2663398686246416)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 181.6490\n",
      "Epoch 00001: val_loss improved from inf to 163.84670, saving model to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 181.2892 - val_loss: 163.8467\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 159.5687\n",
      "Epoch 00002: val_loss improved from 163.84670 to 162.77279, saving model to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 159.3956 - val_loss: 162.7728\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 155.3029\n",
      "Epoch 00003: val_loss improved from 162.77279 to 162.33005, saving model to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 155.1608 - val_loss: 162.3301\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 153.3811\n",
      "Epoch 00004: val_loss improved from 162.33005 to 162.03192, saving model to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 153.2462 - val_loss: 162.0319\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 151.8910\n",
      "Epoch 00005: val_loss improved from 162.03192 to 161.94445, saving model to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 151.7232 - val_loss: 161.9445\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 151.1912\n",
      "Epoch 00006: val_loss improved from 161.94445 to 161.63552, saving model to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 151.0372 - val_loss: 161.6355\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 150.0466\n",
      "Epoch 00007: val_loss improved from 161.63552 to 161.50357, saving model to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 149.9264 - val_loss: 161.5036\n",
      "21/21 [==============================] - 5s 259ms/step\n",
      "** write log to ./experiments/0.19504139478869603_0.013490444119825869__0.19416043034372363_0.3530768597732787_0.2663398686246416_test.log **\n",
      "mean auroc: 0.8568345953078065\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.8359456494426163, 0.19504139478869603, 2.6032301300642007, 0.013490444119825869, 0.19416043034372363, 0.3530768597732787, 0.2663398686246416)\n",
      "Konten (0.8359456494426163, 0.19504139478869603, 2.6032301300642007, 0.013490444119825869, 0.19416043034372363, 0.3530768597732787, 0.2663398686246416)\n",
      "Isine x [1.64777939 0.06642763 2.56594122 0.01360944 0.1934357  0.35132982\n",
      " 0.16057739]\n",
      "Konten (1.6477793946788166, 0.06642762870559298, 2.565941224116222, 0.013609441366594522, 0.19343569586630233, 0.35132981540744873, 0.1605773863053795)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.06642762870559298_0.013609441366594522__0.19343569586630233_0.35132981540744873_0.1605773863053795_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 339)               521043    \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 339)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 339)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 339)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4760      \n",
      "=================================================================\n",
      "Total params: 525,803\n",
      "Trainable params: 525,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -136969.0963\n",
      "Epoch 00001: val_loss improved from inf to -140521.49219, saving model to ./experiments/0.06642762870559298_0.013609441366594522__0.19343569586630233_0.35132981540744873_0.1605773863053795_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: -137022.6325 - val_loss: -140521.4922\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -140425.5181\n",
      "Epoch 00002: val_loss did not improve from -140521.49219\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 22s 296ms/step - loss: -140433.5750 - val_loss: -140521.4922\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -140425.5181\n",
      "Epoch 00003: val_loss did not improve from -140521.49219\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 22s 296ms/step - loss: -140433.5750 - val_loss: -140521.4922\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -140425.5181\n",
      "Epoch 00004: val_loss did not improve from -140521.49219\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "76/76 [==============================] - 22s 296ms/step - loss: -140433.5750 - val_loss: -140521.4922\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -140425.5181\n",
      "Epoch 00005: val_loss did not improve from -140521.49219\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "76/76 [==============================] - 22s 296ms/step - loss: -140433.5750 - val_loss: -140521.4922\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: -140425.5181\n",
      "Epoch 00006: val_loss did not improve from -140521.49219\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 22s 295ms/step - loss: -140433.5750 - val_loss: -140521.4922\n",
      "Epoch 00006: early stopping\n",
      "21/21 [==============================] - 5s 261ms/step\n",
      "** write log to ./experiments/0.06642762870559298_0.013609441366594522__0.19343569586630233_0.35132981540744873_0.1605773863053795_test.log **\n",
      "mean auroc: 0.5\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (1.6477793946788166, 0.06642762870559298, 2.565941224116222, 0.013609441366594522, 0.19343569586630233, 0.35132981540744873, 0.1605773863053795)\n",
      "Konten (1.6477793946788166, 0.06642762870559298, 2.565941224116222, 0.013609441366594522, 0.19343569586630233, 0.35132981540744873, 0.1605773863053795)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=5,acq=\"weightedei\", n_iter=2, omega=0.9)\n",
    "time_took = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0:17:41.41\n",
      "{'target': 0.8576525167957695, 'params': {'alpha': 0.7547456293468533, 'dropout': 0.18172137551441198, 'gamma': 1.2950405014991504, 'lr': 0.011526645750030312, 'neuronPct': 0.1947894476507525, 'neuronShrink': 0.3553316528497302, 'noisePct': 0.30756313418514203}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total runtime: {convert_string(time_took)}\")\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5352"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "params\n"
     ]
    }
   ],
   "source": [
    "new_params = {} \n",
    "for t in optimizer.max.keys():\n",
    "    print(t)\n",
    "    new_params = optimizer.max.get(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = new_params.get('alpha')\n",
    "d = new_params.get('dropout')\n",
    "g = new_params.get('gamma')\n",
    "l = new_params.get('lr')\n",
    "np = new_params.get('neuronPct')\n",
    "ns = new_params.get('neuronShrink')\n",
    "noiP = new_params.get('noisePct')\n",
    "new_params.update({'alpha':(float(a-0.025),float(a+0.025)),\n",
    "                   'dropout':(float(d-0.025),float(d+0.025)),\n",
    "                   'gamma':(float(g-0.025),float(g+0.025)),\n",
    "                   'lr':(float(l),float(l+0.025)),                   \n",
    "                   'neuronPct':(float(np-0.025),float(np+0.025)),\n",
    "                   'neuronShrink':(float(ns-0.025),float(ns+0.025)),\n",
    "                   'noisePct':(float(noiP-0.025),float(noiP+0.025))\n",
    "                  }\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': (0.7297456293468533, 0.7797456293468533),\n",
       " 'dropout': (0.15672137551441198, 0.20672137551441197),\n",
       " 'gamma': (1.2700405014991505, 1.3200405014991503),\n",
       " 'lr': (0.011526645750030312, 0.03652664575003031),\n",
       " 'neuronPct': (0.16978944765075252, 0.2197894476507525),\n",
       " 'neuronShrink': (0.33033165284973015, 0.3803316528497302),\n",
       " 'noisePct': (0.282563134185142, 0.33256313418514205)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "w = csv.writer(open(\"new_params.csv\", \"w\"))\n",
    "for key, val in new_params.items():\n",
    "    w.writerow([key, val])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
