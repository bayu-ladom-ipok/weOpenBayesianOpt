{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(dropout, neuronPct, neuronShrink,noisePct):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neuronCount))\n",
    "    model.add(GaussianNoise(noisePct))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(14, activation='sigmoid')) # Output\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "** class_weights **\n",
      "[{0: 0.976060692178489, 1: 0.023939307821511}, {0: 0.9379028967906056, 1: 0.06209710320939444}, {0: 0.977507900874183, 1: 0.02249209912581691}, {0: 0.9801862148908839, 1: 0.01981378510911613}, {0: 0.9642020357560434, 1: 0.03579796424395662}, {0: 0.9663727015263743, 1: 0.033627298473625666}, {0: 0.8859702012473223, 1: 0.11402979875267771}, {0: 0.9586866934982315, 1: 0.04131330650176841}, {0: 0.9623146440112557, 1: 0.03768535598874437}, {0: 0.9298929992036218, 1: 0.07010700079637826}, {0: 0.9335352709009039, 1: 0.06646472909909606}, {0: 0.9021976306069932, 1: 0.09780236939300682}, {0: 0.9453965277787032, 1: 0.05460347222129675}, {1: 0.720226409263611, 0: 0.27977359073638897}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\", class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\", class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 21 **\n"
     ]
    }
   ],
   "source": [
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    ")\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(dropout,lr,neuronPct,neuronShrink,alpha,gamma,noisePct):\n",
    "      # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir,  str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+str(noisePct)+\"_\"+output_weights_name)\n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "    model = construct_network(dropout, neuronPct, neuronShrink,noisePct)\n",
    "    \n",
    "    #model.compile(loss=focal_loss(gamma=gamma,alpha=alpha), optimizer=SGD(lr=lr))\n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    model.compile(optimizer=optimizer,loss=focal_loss(gamma=gamma,alpha=alpha))\n",
    "    #lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    #lookahead.inject(model) # add into model\n",
    "    output_weights_path = os.path.join(output_dir,  str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+str(noisePct)+\"_\"+output_weights_name)\n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "    model = construct_network(dropout, neuronPct, neuronShrink,noisePct)\n",
    "    \n",
    "    #model.compile(loss=focal_loss(gamma=gamma,alpha=alpha), optimizer=SGD(lr=lr))\n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    model.compile(optimizer=optimizer,loss=focal_loss(gamma=gamma,alpha=alpha))\n",
    "    #lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    #lookahead.inject(model) # add into model\n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, pmatience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+str(noisePct)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    \n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for i in range(len(class_names)):\n",
    "            try:\n",
    "                score = roc_auc_score(y[:, i], y_hat[:, i])\n",
    "                aurocs.append(score)\n",
    "            except ValueError:\n",
    "                score = 0\n",
    "            f.write(f\"{class_names[i]}: {score}\\n\")\n",
    "        mean_auroc = float(np.mean(aurocs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\")\n",
    "    \n",
    "\n",
    "    print(\"iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\")\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    return mean_auroc\n",
    "\n",
    "        \n",
    "    \n",
    "    model.summary()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7297456293468533\n",
      "0.7797456293468533\n",
      "0.15672137551441198\n",
      "0.20672137551441197\n",
      "1.2700405014991505\n",
      "1.3200405014991503\n",
      "0.011526645750030312\n",
      "0.03652664575003031\n",
      "0.16978944765075252\n",
      "0.2197894476507525\n",
      "0.33033165284973015\n",
      "0.3803316528497302\n",
      "0.282563134185142\n",
      "0.33256313418514205\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "pbounds = {}\n",
    "for key, val in csv.reader(open(\"new_params.csv\")):\n",
    "    a,b = val.split(\", \")\n",
    "    a = re.sub('[()]', '', a)\n",
    "    b = re.sub('[()]', '', b)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    pbounds[str(key)] = float(a),float(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': (0.7297456293468533, 0.7797456293468533),\n",
       " 'dropout': (0.15672137551441198, 0.20672137551441197),\n",
       " 'gamma': (1.2700405014991505, 1.3200405014991503),\n",
       " 'lr': (0.011526645750030312, 0.03652664575003031),\n",
       " 'neuronPct': (0.16978944765075252, 0.2197894476507525),\n",
       " 'neuronShrink': (0.33033165284973015, 0.3803316528497302),\n",
       " 'noisePct': (0.282563134185142, 0.33256313418514205)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kunci ['alpha', 'dropout', 'gamma', 'lr', 'neuronPct', 'neuronShrink', 'noisePct']\n",
      "Bound  [[0.72974563 0.77974563]\n",
      " [0.15672138 0.20672138]\n",
      " [1.2700405  1.3200405 ]\n",
      " [0.01152665 0.03652665]\n",
      " [0.16978945 0.21978945]\n",
      " [0.33033165 0.38033165]\n",
      " [0.28256313 0.33256313]]\n"
     ]
    }
   ],
   "source": [
    "from new_bayes_opt.bayesian_optimization import BayesianOptimization\n",
    "import time\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum \n",
    "    # is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konten (1.125533007053861, 0.15804867401632372, 1.0003431244520347, 0.010813995435791039, 0.19073377945408557, 0.35092338594768796, 0.1558780634133013)\n",
      "Konten (1.125533007053861, 0.15804867401632372, 1.0003431244520347, 0.010813995435791039, 0.19073377945408557, 0.35092338594768796, 0.1558780634133013)\n",
      "Konten (1.0183410905645716, 0.1095151211346005, 2.616450202010071, 0.011515167086419769, 0.1934260975019838, 0.35204452249731516, 0.3634352309172837)\n",
      "Konten (1.0183410905645716, 0.1095151211346005, 2.616450202010071, 0.011515167086419769, 0.1934260975019838, 0.35204452249731516, 0.3634352309172837)\n",
      "Konten (0.5410813897968892, 0.15057012652676036, 2.251914407101381, 0.01235213897067451, 0.19070193469297617, 0.35198101489084876, 0.34022337060266106)\n",
      "Konten (0.5410813897968892, 0.15057012652676036, 2.251914407101381, 0.01235213897067451, 0.19070193469297617, 0.35198101489084876, 0.34022337060266106)\n",
      "Konten (1.9523923635790963, 0.09701362672388644, 3.0769678470079422, 0.01425833491377623, 0.19447303331751925, 0.35085044211369776, 0.11171643496986472)\n",
      "Konten (1.9523923635790963, 0.09701362672388644, 3.0769678470079422, 0.01425833491377623, 0.19447303331751925, 0.35085044211369776, 0.11171643496986472)\n",
      "Konten (0.7547456293468533, 0.18172137551441198, 1.2950405014991504, 0.011526645750030312, 0.1947894476507525, 0.3553316528497302, 0.30756313418514203)\n",
      "Konten (0.7547456293468533, 0.18172137551441198, 1.2950405014991504, 0.011526645750030312, 0.1947894476507525, 0.3553316528497302, 0.30756313418514203)\n",
      "Konten (0.8359456494426163, 0.19504139478869603, 2.6032301300642007, 0.013490444119825869, 0.19416043034372363, 0.3530768597732787, 0.2663398686246416)\n",
      "Konten (0.8359456494426163, 0.19504139478869603, 2.6032301300642007, 0.013490444119825869, 0.19416043034372363, 0.3530768597732787, 0.2663398686246416)\n",
      "Konten (1.6477793946788166, 0.06642762870559298, 2.565941224116222, 0.013609441366594522, 0.19343569586630233, 0.35132981540744873, 0.1605773863053795)\n",
      "Konten (1.6477793946788166, 0.06642762870559298, 2.565941224116222, 0.013609441366594522, 0.19343569586630233, 0.35132981540744873, 0.1605773863053795)\n",
      "New optimizer is now aware of 7 points.\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt.util import load_logs#from bayes_opt.util import load_logsm\n",
    "\n",
    "#load_logs(new_optimizer, logs=[\"./noise10_1.json\"])\n",
    "#print(\"New optimizer is now aware of {} points.\".format(len(new_optimizer.space)))\n",
    "\n",
    "load_logs(optimizer, logs=[\"./noise12_1.json\"])\n",
    "print(\"New optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "new_logger = JSONLogger(path=\"./noise12_2.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isine x [0.75059673 0.1927376  1.27004622 0.01908496 0.17712724 0.33494858\n",
      " 0.29187614]\n",
      "Konten (0.750596729581982, 0.19273760018651986, 1.2700462202400178, 0.019084960065826308, 0.17712724219160816, 0.33494858258817006, 0.29187614475402557)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5 **\n",
      "** set output weights path to: ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5 **\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 296)               454952    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 296)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 296)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 296)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                4158      \n",
      "=================================================================\n",
      "Total params: 459,110\n",
      "Trainable params: 459,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-8-c986a89f80c1>:58: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 471.1520\n",
      "Epoch 00001: val_loss improved from inf to 445.29671, saving model to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 470.1561 - val_loss: 445.2967\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 417.6750\n",
      "Epoch 00002: val_loss improved from 445.29671 to 442.78825, saving model to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 417.2319 - val_loss: 442.7883\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 410.6286\n",
      "Epoch 00003: val_loss improved from 442.78825 to 441.08876, saving model to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 410.1679 - val_loss: 441.0888\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 407.0352\n",
      "Epoch 00004: val_loss improved from 441.08876 to 440.32241, saving model to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 406.6291 - val_loss: 440.3224\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 405.0925\n",
      "Epoch 00005: val_loss improved from 440.32241 to 440.14324, saving model to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 404.6747 - val_loss: 440.1432\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 403.5905\n",
      "Epoch 00006: val_loss improved from 440.14324 to 439.27020, saving model to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 403.1381 - val_loss: 439.2702\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 401.4270\n",
      "Epoch 00007: val_loss improved from 439.27020 to 438.72433, saving model to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 401.0606 - val_loss: 438.7243\n",
      "WARNING:tensorflow:From <ipython-input-8-c986a89f80c1>:60: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "21/21 [==============================] - 6s 263ms/step\n",
      "** write log to ./experiments/0.19273760018651986_0.019084960065826308__0.17712724219160816_0.33494858258817006_0.29187614475402557_test.log **\n",
      "mean auroc: 0.8584130522200262\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.750596729581982, 0.19273760018651986, 1.2700462202400178, 0.019084960065826308, 0.17712724219160816, 0.33494858258817006, 0.29187614475402557)\n",
      "Konten (0.750596729581982, 0.19273760018651986, 1.2700462202400178, 0.019084960065826308, 0.17712724219160816, 0.33494858258817006, 0.29187614475402557)\n",
      "Isine x [0.74702367 0.17655975 1.29698134 0.02200651 0.20405042 0.34055427\n",
      " 0.32646901]\n",
      "Konten (0.7470236656990057, 0.17655974922594547, 1.2969813381993183, 0.022006508610112684, 0.20405042267059048, 0.340554265336306, 0.3264690060046893)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5 **\n",
      "** set output weights path to: ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5 **\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 347)               533339    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 347)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 347)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 347)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                4872      \n",
      "=================================================================\n",
      "Total params: 538,211\n",
      "Trainable params: 538,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 472.6162\n",
      "Epoch 00001: val_loss improved from inf to 442.73636, saving model to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 471.5590 - val_loss: 442.7364\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 413.1698\n",
      "Epoch 00002: val_loss improved from 442.73636 to 439.67400, saving model to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 412.6594 - val_loss: 439.6740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 406.0761\n",
      "Epoch 00003: val_loss improved from 439.67400 to 437.86410, saving model to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 405.5690 - val_loss: 437.8641\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 403.3283\n",
      "Epoch 00004: val_loss improved from 437.86410 to 437.31690, saving model to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 402.9336 - val_loss: 437.3169\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 400.4683\n",
      "Epoch 00005: val_loss improved from 437.31690 to 436.43507, saving model to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 399.9583 - val_loss: 436.4351\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 398.8179\n",
      "Epoch 00006: val_loss improved from 436.43507 to 435.69128, saving model to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 398.4151 - val_loss: 435.6913\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 397.2469\n",
      "Epoch 00007: val_loss improved from 435.69128 to 435.45381, saving model to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 396.8675 - val_loss: 435.4538\n",
      "21/21 [==============================] - 5s 261ms/step\n",
      "** write log to ./experiments/0.17655974922594547_0.022006508610112684__0.20405042267059048_0.340554265336306_0.3264690060046893_test.log **\n",
      "mean auroc: 0.8574933418343805\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.7470236656990057, 0.17655974922594547, 1.2969813381993183, 0.022006508610112684, 0.20405042267059048, 0.340554265336306, 0.3264690060046893)\n",
      "Konten (0.7470236656990057, 0.17655974922594547, 1.2969813381993183, 0.022006508610112684, 0.20405042267059048, 0.340554265336306, 0.3264690060046893)\n",
      "Isine x [0.73111501 0.19024475 1.29090574 0.02549389 0.17680879 0.34023673\n",
      " 0.32260036]\n",
      "Konten (0.7311150090067496, 0.19024475102333208, 1.2909057416175067, 0.025493891461174104, 0.1768087945805142, 0.3402367273039741, 0.3226003626189189)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5 **\n",
      "** set output weights path to: ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5 **\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               461100    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                4214      \n",
      "=================================================================\n",
      "Total params: 465,314\n",
      "Trainable params: 465,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 481.0760\n",
      "Epoch 00001: val_loss improved from inf to 447.34364, saving model to ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 480.0567 - val_loss: 447.3436\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 421.4915\n",
      "Epoch 00002: val_loss improved from 447.34364 to 443.71196, saving model to ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 420.9246 - val_loss: 443.7120\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 413.3746\n",
      "Epoch 00003: val_loss improved from 443.71196 to 442.81184, saving model to ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 412.9361 - val_loss: 442.8118\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 409.8953\n",
      "Epoch 00004: val_loss improved from 442.81184 to 441.72503, saving model to ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 409.5018 - val_loss: 441.7250\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 406.9642\n",
      "Epoch 00005: val_loss improved from 441.72503 to 441.20548, saving model to ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 406.6052 - val_loss: 441.2055\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 405.3160\n",
      "Epoch 00006: val_loss did not improve from 441.20548\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 404.8990 - val_loss: 441.2655\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 404.1672\n",
      "Epoch 00007: val_loss improved from 441.20548 to 439.82899, saving model to ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_weights.h5\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 403.7707 - val_loss: 439.8290\n",
      "21/21 [==============================] - 5s 261ms/step\n",
      "** write log to ./experiments/0.19024475102333208_0.025493891461174104__0.1768087945805142_0.3402367273039741_0.3226003626189189_test.log **\n",
      "mean auroc: 0.8579733360834938\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.7311150090067496, 0.19024475102333208, 1.2909057416175067, 0.025493891461174104, 0.1768087945805142, 0.3402367273039741, 0.3226003626189189)\n",
      "Konten (0.7311150090067496, 0.19024475102333208, 1.2909057416175067, 0.025493891461174104, 0.1768087945805142, 0.3402367273039741, 0.3226003626189189)\n",
      "Isine x [0.77815871 0.17239258 1.30465663 0.03343637 0.21451978 0.33458386\n",
      " 0.28451587]\n",
      "Konten (0.7781587081328232, 0.17239258442237412, 1.304656632282616, 0.03343637455743127, 0.21451978082594486, 0.33458386341821905, 0.28451587334678613)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5 **\n",
      "** set output weights path to: ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5 **\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 358)               550246    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 358)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 358)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 358)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                5026      \n",
      "=================================================================\n",
      "Total params: 555,272\n",
      "Trainable params: 555,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 449.3598\n",
      "Epoch 00001: val_loss improved from inf to 422.25827, saving model to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 448.4001 - val_loss: 422.2583\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 396.3787\n",
      "Epoch 00002: val_loss improved from 422.25827 to 419.93888, saving model to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 395.9893 - val_loss: 419.9389\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 389.9197\n",
      "Epoch 00003: val_loss improved from 419.93888 to 419.58265, saving model to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 389.5342 - val_loss: 419.5827\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 386.1175\n",
      "Epoch 00004: val_loss improved from 419.58265 to 418.49728, saving model to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 385.7097 - val_loss: 418.4973\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 383.7850\n",
      "Epoch 00005: val_loss improved from 418.49728 to 418.26571, saving model to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 383.3362 - val_loss: 418.2657\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 382.4883\n",
      "Epoch 00006: val_loss improved from 418.26571 to 418.19558, saving model to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 382.1214 - val_loss: 418.1956\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 380.9729\n",
      "Epoch 00007: val_loss improved from 418.19558 to 417.62328, saving model to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 380.5787 - val_loss: 417.6233\n",
      "21/21 [==============================] - 5s 262ms/step\n",
      "** write log to ./experiments/0.17239258442237412_0.03343637455743127__0.21451978082594486_0.33458386341821905_0.28451587334678613_test.log **\n",
      "mean auroc: 0.858427929314195\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.7781587081328232, 0.17239258442237412, 1.304656632282616, 0.03343637455743127, 0.21451978082594486, 0.33458386341821905, 0.28451587334678613)\n",
      "Konten (0.7781587081328232, 0.17239258442237412, 1.304656632282616, 0.03343637455743127, 0.21451978082594486, 0.33458386341821905, 0.28451587334678613)\n",
      "Isine x [0.73823715 0.2006285  1.27495784 0.02205434 0.21768392 0.35698992\n",
      " 0.31715699]\n",
      "Konten (0.7382371503250817, 0.20062850068588262, 1.274957843190803, 0.022054336375156615, 0.2176839241582776, 0.356989917098381, 0.3171569898826657)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5 **\n",
      "** set output weights path to: ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5 **\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 388)               596356    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 388)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 388)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 388)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                5446      \n",
      "=================================================================\n",
      "Total params: 601,802\n",
      "Trainable params: 601,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 481.3904\n",
      "Epoch 00001: val_loss improved from inf to 458.62924, saving model to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 480.2702 - val_loss: 458.6292\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 422.8777\n",
      "Epoch 00002: val_loss improved from 458.62924 to 453.76272, saving model to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 422.3964 - val_loss: 453.7627\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 414.9315\n",
      "Epoch 00003: val_loss improved from 453.76272 to 452.89410, saving model to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 414.4934 - val_loss: 452.8941\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 411.2696\n",
      "Epoch 00004: val_loss improved from 452.89410 to 451.14180, saving model to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 410.8790 - val_loss: 451.1418\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 408.7411\n",
      "Epoch 00005: val_loss improved from 451.14180 to 449.85779, saving model to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 408.3240 - val_loss: 449.8578\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 406.8700\n",
      "Epoch 00006: val_loss improved from 449.85779 to 449.29565, saving model to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 406.4194 - val_loss: 449.2956\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 405.7014\n",
      "Epoch 00007: val_loss improved from 449.29565 to 447.96335, saving model to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 405.2742 - val_loss: 447.9634\n",
      "21/21 [==============================] - 5s 261ms/step\n",
      "** write log to ./experiments/0.20062850068588262_0.022054336375156615__0.2176839241582776_0.356989917098381_0.3171569898826657_test.log **\n",
      "mean auroc: 0.85808555480924\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.7382371503250817, 0.20062850068588262, 1.274957843190803, 0.022054336375156615, 0.2176839241582776, 0.356989917098381, 0.3171569898826657)\n",
      "Konten (0.7382371503250817, 0.20062850068588262, 1.274957843190803, 0.022054336375156615, 0.2176839241582776, 0.356989917098381, 0.3171569898826657)\n",
      "Isine x [0.72974563 0.15672138 1.2700405  0.03652665 0.21978945 0.38033165\n",
      " 0.28256313]\n",
      "Konten (0.7297456293468533, 0.15672137551441198, 1.2700405014991505, 0.03652664575003031, 0.2197894476507525, 0.3803316528497302, 0.282563134185142)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5 **\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** set output weights path to: ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5 **\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 417)               640929    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 417)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 417)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 417)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                5852      \n",
      "=================================================================\n",
      "Total params: 646,781\n",
      "Trainable params: 646,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 477.1596\n",
      "Epoch 00001: val_loss improved from inf to 455.50410, saving model to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 476.0051 - val_loss: 455.5041\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 421.7259\n",
      "Epoch 00002: val_loss improved from 455.50410 to 452.79240, saving model to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 421.2522 - val_loss: 452.7924\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 414.0571\n",
      "Epoch 00003: val_loss improved from 452.79240 to 452.06939, saving model to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 413.5997 - val_loss: 452.0694\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 410.8111\n",
      "Epoch 00004: val_loss improved from 452.06939 to 450.92994, saving model to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 410.3362 - val_loss: 450.9299\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 408.8983\n",
      "Epoch 00005: val_loss improved from 450.92994 to 450.48836, saving model to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 408.4848 - val_loss: 450.4884\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 407.2289\n",
      "Epoch 00006: val_loss improved from 450.48836 to 448.83638, saving model to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 406.7451 - val_loss: 448.8364\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 405.9686\n",
      "Epoch 00007: val_loss improved from 448.83638 to 448.58460, saving model to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 405.5464 - val_loss: 448.5846\n",
      "21/21 [==============================] - 6s 262ms/step\n",
      "** write log to ./experiments/0.15672137551441198_0.03652664575003031__0.2197894476507525_0.3803316528497302_0.282563134185142_test.log **\n",
      "mean auroc: 0.8584571381663927\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.7297456293468533, 0.15672137551441198, 1.2700405014991505, 0.03652664575003031, 0.2197894476507525, 0.3803316528497302, 0.282563134185142)\n",
      "Konten (0.7297456293468533, 0.15672137551441198, 1.2700405014991505, 0.03652664575003031, 0.2197894476507525, 0.3803316528497302, 0.282563134185142)\n",
      "Isine x [0.76800494 0.16219725 1.29613952 0.03073265 0.20414641 0.33698073\n",
      " 0.29265937]\n",
      "Konten (0.7680049425028138, 0.16219725174960964, 1.2961395219010874, 0.030732651444174162, 0.20414640631377562, 0.336980729886974, 0.2926593652360386)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5 **\n",
      "** set output weights path to: ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5 **\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 343)               527191    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 343)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 343)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 343)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                4816      \n",
      "=================================================================\n",
      "Total params: 532,007\n",
      "Trainable params: 532,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 457.5536\n",
      "Epoch 00001: val_loss improved from inf to 432.06549, saving model to ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 456.4795 - val_loss: 432.0655\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 403.0008\n",
      "Epoch 00002: val_loss improved from 432.06549 to 429.99330, saving model to ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 402.5283 - val_loss: 429.9933\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 396.8125\n",
      "Epoch 00003: val_loss improved from 429.99330 to 428.54532, saving model to ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 396.3932 - val_loss: 428.5453\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 392.3360\n",
      "Epoch 00004: val_loss improved from 428.54532 to 427.06631, saving model to ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 391.8726 - val_loss: 427.0663\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 390.3467\n",
      "Epoch 00005: val_loss improved from 427.06631 to 426.88263, saving model to ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 389.9071 - val_loss: 426.8826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 388.7100\n",
      "Epoch 00006: val_loss improved from 426.88263 to 426.59942, saving model to ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 388.2328 - val_loss: 426.5994\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 387.2755\n",
      "Epoch 00007: val_loss did not improve from 426.59942\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 386.8209 - val_loss: 426.8912\n",
      "21/21 [==============================] - 5s 260ms/step\n",
      "** write log to ./experiments/0.16219725174960964_0.030732651444174162__0.20414640631377562_0.336980729886974_0.2926593652360386_test.log **\n",
      "mean auroc: 0.8584102017902405\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink|noisePct\n",
      "Konten (0.7680049425028138, 0.16219725174960964, 1.2961395219010874, 0.030732651444174162, 0.20414640631377562, 0.336980729886974, 0.2926593652360386)\n",
      "Konten (0.7680049425028138, 0.16219725174960964, 1.2961395219010874, 0.030732651444174162, 0.20414640631377562, 0.336980729886974, 0.2926593652360386)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=5,acq=\"weightedei\", n_iter=2, omega=0.1)\n",
    "time_took = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0:19:18.23\n",
      "{'target': 0.8584571381663927, 'params': {'alpha': 0.7297456293468533, 'dropout': 0.15672137551441198, 'gamma': 1.2700405014991505, 'lr': 0.03652664575003031, 'neuronPct': 0.2197894476507525, 'neuronShrink': 0.3803316528497302, 'noisePct': 0.282563134185142}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total runtime: {convert_string(time_took)}\")\n",
    "print(optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
