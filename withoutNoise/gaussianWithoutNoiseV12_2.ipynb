{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(dropout, neuronPct, neuronShrink):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neuronCount))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(14, activation='sigmoid')) # Output\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "** class_weights **\n",
      "[{0: 0.976060692178489, 1: 0.023939307821511}, {0: 0.9379028967906056, 1: 0.06209710320939444}, {0: 0.977507900874183, 1: 0.02249209912581691}, {0: 0.9801862148908839, 1: 0.01981378510911613}, {0: 0.9642020357560434, 1: 0.03579796424395662}, {0: 0.9663727015263743, 1: 0.033627298473625666}, {0: 0.8859702012473223, 1: 0.11402979875267771}, {0: 0.9586866934982315, 1: 0.04131330650176841}, {0: 0.9623146440112557, 1: 0.03768535598874437}, {0: 0.9298929992036218, 1: 0.07010700079637826}, {0: 0.9335352709009039, 1: 0.06646472909909606}, {0: 0.9021976306069932, 1: 0.09780236939300682}, {0: 0.9453965277787032, 1: 0.05460347222129675}, {1: 0.720226409263611, 0: 0.27977359073638897}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\", class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\", class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 21 **\n"
     ]
    }
   ],
   "source": [
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    ")\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(dropout,lr,neuronPct,neuronShrink,alpha,gamma):\n",
    "      # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir,  str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+output_weights_name)\n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "    model = construct_network(dropout, neuronPct, neuronShrink)\n",
    "    \n",
    "    #model.compile(loss=focal_loss(gamma=gamma,alpha=alpha), optimizer=SGD(lr=lr))\n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    model.compile(optimizer=optimizer,loss=focal_loss(gamma=gamma,alpha=alpha))\n",
    "    #lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    #lookahead.inject(model) # add into model\n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, pmatience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    \n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for i in range(len(class_names)):\n",
    "            try:\n",
    "                score = roc_auc_score(y[:, i], y_hat[:, i])\n",
    "                aurocs.append(score)\n",
    "            except ValueError:\n",
    "                score = 0\n",
    "            f.write(f\"{class_names[i]}: {score}\\n\")\n",
    "        mean_auroc = float(np.mean(aurocs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\")\n",
    "    \n",
    "\n",
    "    print(\"iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\")\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    return mean_auroc\n",
    "\n",
    "        \n",
    "    \n",
    "    model.summary()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9131945761480191\n",
      "0.9631945761480192\n",
      "0.04823894606663504\n",
      "0.09823894606663505\n",
      "1.4195751593286225\n",
      "1.4695751593286224\n",
      "0.00923039054783233\n",
      "0.034230390547832334\n",
      "0.16941698304195646\n",
      "0.21941698304195645\n",
      "0.32718781425034293\n",
      "0.377187814250343\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "pbounds = {}\n",
    "for key, val in csv.reader(open(\"new_params.csv\")):\n",
    "    a,b = val.split(\", \")\n",
    "    a = re.sub('[()]', '', a)\n",
    "    b = re.sub('[()]', '', b)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    pbounds[str(key)] = float(a),float(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': (0.9131945761480191, 0.9631945761480192),\n",
       " 'dropout': (0.04823894606663504, 0.09823894606663505),\n",
       " 'gamma': (1.4195751593286225, 1.4695751593286224),\n",
       " 'lr': (0.00923039054783233, 0.034230390547832334),\n",
       " 'neuronPct': (0.16941698304195646, 0.21941698304195645),\n",
       " 'neuronShrink': (0.32718781425034293, 0.377187814250343)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kunci ['alpha', 'dropout', 'gamma', 'lr', 'neuronPct', 'neuronShrink']\n",
      "Bound  [[0.91319458 0.96319458]\n",
      " [0.04823895 0.09823895]\n",
      " [1.41957516 1.46957516]\n",
      " [0.00923039 0.03423039]\n",
      " [0.16941698 0.21941698]\n",
      " [0.32718781 0.37718781]]\n"
     ]
    }
   ],
   "source": [
    "from new_bayes_opt.bayesian_optimization import BayesianOptimization\n",
    "import time\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum \n",
    "    # is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konten (0.7085110023512871, 0.07323720324493442, 1.3001944371894862, 0.009233023325726319, 0.1944146755890817, 0.3521092338594769)\n",
      "Konten (0.7085110023512871, 0.07323720324493442, 1.3001944371894862, 0.009233023325726319, 0.1944146755890817, 0.3521092338594769)\n",
      "Konten (0.5931301056888354, 0.07323345560727043, 1.974504706192139, 0.009235388167340033, 0.19444191945144032, 0.35216852195003967)\n",
      "Konten (0.5931301056888354, 0.07323345560727043, 1.974504706192139, 0.009235388167340033, 0.19444191945144032, 0.35216852195003967)\n",
      "Konten (0.6022261248657588, 0.07323878117436391, 1.3465589084364744, 0.009236704675101784, 0.19444173048023672, 0.35215586898284457)\n",
      "Konten (0.6022261248657588, 0.07323878117436391, 1.3465589084364744, 0.009236704675101784, 0.19444173048023672, 0.35215586898284457)\n",
      "Konten (0.5701934692976169, 0.07323198101489085, 2.6612657667484125, 0.009239682615757194, 0.19443134241781593, 0.35216923226156693)\n",
      "Konten (0.5701934692976169, 0.07323198101489085, 2.6612657667484125, 0.009239682615757194, 0.19443134241781593, 0.35216923226156693)\n",
      "Konten (0.9381945761480192, 0.07323894606663504, 1.4445751593286225, 0.00923039054783233, 0.19441698304195645, 0.35218781425034296)\n",
      "Konten (0.9381945761480192, 0.07323894606663504, 1.4445751593286225, 0.00923039054783233, 0.19441698304195645, 0.35218781425034296)\n",
      "Konten (0.9444797518217164, 0.07323864575471516, 1.448890713827503, 0.00923, 0.1944198563064689, 0.3521895588218734)\n",
      "Konten (0.9444797518217164, 0.07323864575471516, 1.448890713827503, 0.00923, 0.1944198563064689, 0.3521895588218734)\n",
      "Konten (0.7065159177626684, 0.07323368750657194, 1.300869428557757, 0.009238339734124617, 0.1944562640324099, 0.35216793540535646)\n",
      "Konten (0.7065159177626684, 0.07323368750657194, 1.300869428557757, 0.009238339734124617, 0.1944562640324099, 0.35216793540535646)\n",
      "New optimizer is now aware of 7 points.\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt.util import load_logs#from bayes_opt.util import load_logsm\n",
    "\n",
    "#load_logs(new_optimizer, logs=[\"./noise10_1.json\"])\n",
    "#print(\"New optimizer is now aware of {} points.\".format(len(new_optimizer.space)))\n",
    "\n",
    "load_logs(optimizer, logs=[\"./without_noise12_1.json\"])\n",
    "print(\"New optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "new_logger = JSONLogger(path=\"./without_noise12_2.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isine x [0.93404568 0.08425517 1.41958088 0.0167887  0.17675478 0.33180474]\n",
      "Konten (0.9340456763831478, 0.08425517073874295, 1.4195808780694898, 0.016788704863628323, 0.1767547775828121, 0.33180474398878285)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.08425517073874295_0.016788704863628323__0.1767547775828121_0.33180474398878285_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 292)               448804    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 292)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 292)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4102      \n",
      "=================================================================\n",
      "Total params: 452,906\n",
      "Trainable params: 452,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-8-9bfbac42c224>:42: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 245.7787\n",
      "Epoch 00001: val_loss improved from inf to 249.26804, saving model to ./experiments/0.08425517073874295_0.016788704863628323__0.1767547775828121_0.33180474398878285_weights.h5\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 245.2631 - val_loss: 249.2680\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 220.7912\n",
      "Epoch 00002: val_loss did not improve from 249.26804\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 220.5378 - val_loss: 249.7749\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 218.0316\n",
      "Epoch 00003: val_loss did not improve from 249.26804\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 217.8193 - val_loss: 249.6232\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 216.7453\n",
      "Epoch 00004: val_loss improved from 249.26804 to 248.98795, saving model to ./experiments/0.08425517073874295_0.016788704863628323__0.1767547775828121_0.33180474398878285_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 216.5156 - val_loss: 248.9880\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 215.9583\n",
      "Epoch 00005: val_loss improved from 248.98795 to 248.85627, saving model to ./experiments/0.08425517073874295_0.016788704863628323__0.1767547775828121_0.33180474398878285_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 215.7578 - val_loss: 248.8563\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 215.0705\n",
      "Epoch 00006: val_loss did not improve from 248.85627\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 214.8585 - val_loss: 249.1021\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 214.5111\n",
      "Epoch 00007: val_loss did not improve from 248.85627\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 214.3083 - val_loss: 249.1428\n",
      "WARNING:tensorflow:From <ipython-input-8-9bfbac42c224>:44: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "21/21 [==============================] - 5s 260ms/step\n",
      "** write log to ./experiments/0.08425517073874295_0.016788704863628323__0.1767547775828121_0.33180474398878285_test.log **\n",
      "mean auroc: 0.8608915704545345\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9340456763831478, 0.08425517073874295, 1.4195808780694898, 0.016788704863628323, 0.1767547775828121, 0.33180474398878285)\n",
      "Konten (0.9340456763831478, 0.08425517073874295, 1.4195808780694898, 0.016788704863628323, 0.1767547775828121, 0.33180474398878285)\n",
      "Isine x [0.92250759 0.06551698 1.43941353 0.02270081 0.19037671 0.36144879]\n",
      "Konten (0.9225075867169027, 0.06551698241878744, 1.439413533040156, 0.022700808897916258, 0.1903767087621212, 0.3614487892701809)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.06551698241878744_0.022700808897916258__0.1903767087621212_0.3614487892701809_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 343)               527191    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 343)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 343)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4816      \n",
      "=================================================================\n",
      "Total params: 532,007\n",
      "Trainable params: 532,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 257.9618\n",
      "Epoch 00001: val_loss improved from inf to 263.92384, saving model to ./experiments/0.06551698241878744_0.022700808897916258__0.1903767087621212_0.3614487892701809_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 257.4442 - val_loss: 263.9238\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 233.6505\n",
      "Epoch 00002: val_loss did not improve from 263.92384\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 233.3955 - val_loss: 263.9430\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 230.7275\n",
      "Epoch 00003: val_loss did not improve from 263.92384\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 230.4813 - val_loss: 264.2901\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 229.4981\n",
      "Epoch 00004: val_loss did not improve from 263.92384\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 229.2528 - val_loss: 264.4831\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 228.8154\n",
      "Epoch 00005: val_loss did not improve from 263.92384\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 228.5756 - val_loss: 264.3706\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 228.1487\n",
      "Epoch 00006: val_loss did not improve from 263.92384\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 227.9117 - val_loss: 264.2640\n",
      "Epoch 00006: early stopping\n",
      "21/21 [==============================] - 5s 260ms/step\n",
      "** write log to ./experiments/0.06551698241878744_0.022700808897916258__0.1903767087621212_0.3614487892701809_test.log **\n",
      "mean auroc: 0.8565194255980433\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9225075867169027, 0.06551698241878744, 1.439413533040156, 0.022700808897916258, 0.1903767087621212, 0.3614487892701809)\n",
      "Konten (0.9225075867169027, 0.06551698241878744, 1.439413533040156, 0.022700808897916258, 0.1903767087621212, 0.3614487892701809)\n",
      "Isine x [0.92341719 0.09214482 1.42094454 0.02599208 0.19028222 0.35512231]\n",
      "Konten (0.923417188634595, 0.09214481788618231, 1.420944538988519, 0.025992078302292387, 0.1902822231603128, 0.35512230567263053)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.09214481788618231_0.025992078302292387__0.1902822231603128_0.35512230567263053_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 337)               517969    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 337)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 337)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4732      \n",
      "=================================================================\n",
      "Total params: 522,701\n",
      "Trainable params: 522,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 261.8456\n",
      "Epoch 00001: val_loss improved from inf to 265.93679, saving model to ./experiments/0.09214481788618231_0.025992078302292387__0.1902822231603128_0.35512230567263053_weights.h5\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 261.3278 - val_loss: 265.9368\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 235.9441\n",
      "Epoch 00002: val_loss improved from 265.93679 to 265.44559, saving model to ./experiments/0.09214481788618231_0.025992078302292387__0.1902822231603128_0.35512230567263053_weights.h5\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 235.7172 - val_loss: 265.4456\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 232.8049\n",
      "Epoch 00003: val_loss did not improve from 265.44559\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 232.5690 - val_loss: 265.5396\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 231.5457\n",
      "Epoch 00004: val_loss did not improve from 265.44559\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 231.3218 - val_loss: 265.9959\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 230.7827\n",
      "Epoch 00005: val_loss did not improve from 265.44559\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 230.5415 - val_loss: 265.8044\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 230.1607\n",
      "Epoch 00006: val_loss did not improve from 265.44559\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 229.9243 - val_loss: 265.8093\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 229.6213\n",
      "Epoch 00007: val_loss did not improve from 265.44559\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 229.3938 - val_loss: 266.0529\n",
      "Epoch 00007: early stopping\n",
      "21/21 [==============================] - 5s 258ms/step\n",
      "** write log to ./experiments/0.09214481788618231_0.025992078302292387__0.1902822231603128_0.35512230567263053_test.log **\n",
      "mean auroc: 0.8583448512041937\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.923417188634595, 0.09214481788618231, 1.420944538988519, 0.025992078302292387, 0.1902822231603128, 0.35512230567263053)\n",
      "Konten (0.923417188634595, 0.09214481788618231, 1.420944538988519, 0.025992078302292387, 0.1902822231603128, 0.35512230567263053)\n",
      "Isine x [0.92021392 0.05814402 1.45961239 0.03343693 0.18508819 0.36180395]\n",
      "Konten (0.9202139230777808, 0.05814402052087898, 1.4596123877623992, 0.033436929940817275, 0.1850881919499186, 0.36180394503380864)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.05814402052087898_0.033436929940817275__0.1850881919499186_0.36180394503380864_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 334)               513358    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 334)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 334)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4690      \n",
      "=================================================================\n",
      "Total params: 518,048\n",
      "Trainable params: 518,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 258.3937\n",
      "Epoch 00001: val_loss improved from inf to 263.66081, saving model to ./experiments/0.05814402052087898_0.033436929940817275__0.1850881919499186_0.36180394503380864_weights.h5\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 257.8737 - val_loss: 263.6608\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 233.3593\n",
      "Epoch 00002: val_loss did not improve from 263.66081\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 233.1155 - val_loss: 263.8911\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 230.4992\n",
      "Epoch 00003: val_loss did not improve from 263.66081\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 230.2590 - val_loss: 264.4586\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 229.3090\n",
      "Epoch 00004: val_loss did not improve from 263.66081\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 229.0575 - val_loss: 264.3120\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 228.6344\n",
      "Epoch 00005: val_loss did not improve from 263.66081\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 228.3985 - val_loss: 264.3490\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 227.9132\n",
      "Epoch 00006: val_loss did not improve from 263.66081\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 227.6743 - val_loss: 264.3883\n",
      "Epoch 00006: early stopping\n",
      "21/21 [==============================] - 5s 258ms/step\n",
      "** write log to ./experiments/0.05814402052087898_0.033436929940817275__0.1850881919499186_0.36180394503380864_test.log **\n",
      "mean auroc: 0.8558945606779726\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9202139230777808, 0.05814402052087898, 1.4596123877623992, 0.033436929940817275, 0.1850881919499186, 0.36180394503380864)\n",
      "Konten (0.9202139230777808, 0.05814402052087898, 1.4596123877623992, 0.033436929940817275, 0.1850881919499186, 0.36180394503380864)\n",
      "Isine x [0.95701403 0.09296928 1.42382737 0.01020676 0.1779085  0.37109494]\n",
      "Konten (0.9570140337628211, 0.09296927924182741, 1.4238273698971113, 0.010206760128654388, 0.1779085040201849, 0.37109493942181365)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.09296927924182741_0.010206760128654388__0.1779085040201849_0.37109493942181365_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 329)               505673    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 329)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 329)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4620      \n",
      "=================================================================\n",
      "Total params: 510,293\n",
      "Trainable params: 510,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 203.0044\n",
      "Epoch 00001: val_loss improved from inf to 203.04481, saving model to ./experiments/0.09296927924182741_0.010206760128654388__0.1779085040201849_0.37109493942181365_weights.h5\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 202.5718 - val_loss: 203.0448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 179.5154\n",
      "Epoch 00002: val_loss improved from 203.04481 to 202.54418, saving model to ./experiments/0.09296927924182741_0.010206760128654388__0.1779085040201849_0.37109493942181365_weights.h5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 179.3251 - val_loss: 202.5442\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 176.9688\n",
      "Epoch 00003: val_loss did not improve from 202.54418\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 176.7982 - val_loss: 202.6085\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 175.8156\n",
      "Epoch 00004: val_loss did not improve from 202.54418\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 175.6389 - val_loss: 202.6874\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 174.9321\n",
      "Epoch 00005: val_loss did not improve from 202.54418\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 174.7535 - val_loss: 202.6050\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 174.6020\n",
      "Epoch 00006: val_loss did not improve from 202.54418\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 174.4323 - val_loss: 202.5770\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 174.1845\n",
      "Epoch 00007: val_loss did not improve from 202.54418\n",
      "Restoring model weights from the end of the best epoch.\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 174.0197 - val_loss: 202.5845\n",
      "Epoch 00007: early stopping\n",
      "21/21 [==============================] - 5s 258ms/step\n",
      "** write log to ./experiments/0.09296927924182741_0.010206760128654388__0.1779085040201849_0.37109493942181365_test.log **\n",
      "mean auroc: 0.858853181307459\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9570140337628211, 0.09296927924182741, 1.4238273698971113, 0.010206760128654388, 0.1779085040201849, 0.37109493942181365)\n",
      "Konten (0.9570140337628211, 0.09296927924182741, 1.4238273698971113, 0.010206760128654388, 0.1779085040201849, 0.37109493942181365)\n",
      "Isine x [0.93831785 0.08113881 1.42739872 0.01138402 0.18138056 0.334046  ]\n",
      "Konten (0.9383178538336772, 0.08113880950675538, 1.4273987159339294, 0.011384017046286625, 0.18138055785099064, 0.3340460016329543)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.08113880950675538_0.011384017046286625__0.18138055785099064_0.3340460016329543_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 302)               464174    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 302)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 302)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4242      \n",
      "=================================================================\n",
      "Total params: 468,416\n",
      "Trainable params: 468,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 235.4387\n",
      "Epoch 00001: val_loss improved from inf to 240.87553, saving model to ./experiments/0.08113880950675538_0.011384017046286625__0.18138055785099064_0.3340460016329543_weights.h5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 234.9713 - val_loss: 240.8755\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 212.5188\n",
      "Epoch 00002: val_loss improved from 240.87553 to 240.43899, saving model to ./experiments/0.08113880950675538_0.011384017046286625__0.18138055785099064_0.3340460016329543_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 212.3131 - val_loss: 240.4390\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 209.9335\n",
      "Epoch 00003: val_loss improved from 240.43899 to 240.04571, saving model to ./experiments/0.08113880950675538_0.011384017046286625__0.18138055785099064_0.3340460016329543_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 209.7317 - val_loss: 240.0457\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 208.7441\n",
      "Epoch 00004: val_loss did not improve from 240.04571\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 208.5279 - val_loss: 240.1998\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 208.1095\n",
      "Epoch 00005: val_loss did not improve from 240.04571\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 207.8809 - val_loss: 240.3472\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 207.4977\n",
      "Epoch 00006: val_loss did not improve from 240.04571\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 207.2777 - val_loss: 240.2390\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 207.0223\n",
      "Epoch 00007: val_loss did not improve from 240.04571\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 206.8064 - val_loss: 240.2444\n",
      "21/21 [==============================] - 5s 257ms/step\n",
      "** write log to ./experiments/0.08113880950675538_0.011384017046286625__0.18138055785099064_0.3340460016329543_test.log **\n",
      "mean auroc: 0.8602543975832332\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9383178538336772, 0.08113880950675538, 1.4273987159339294, 0.011384017046286625, 0.18138055785099064, 0.3340460016329543)\n",
      "Konten (0.9383178538336772, 0.08113880950675538, 1.4273987159339294, 0.011384017046286625, 0.18138055785099064, 0.3340460016329543)\n",
      "Isine x [0.94010888 0.08926782 1.41957516 0.01160319 0.16941698 0.32718781]\n",
      "Konten (0.9401088790363802, 0.08926782252449214, 1.4195751593286225, 0.011603194685843438, 0.16941698304195646, 0.32718781425034293)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.08926782252449214_0.011603194685843438__0.16941698304195646_0.32718781425034293_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 277)               425749    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 277)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 277)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                3892      \n",
      "=================================================================\n",
      "Total params: 429,641\n",
      "Trainable params: 429,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 235.5262\n",
      "Epoch 00001: val_loss improved from inf to 238.24022, saving model to ./experiments/0.08926782252449214_0.011603194685843438__0.16941698304195646_0.32718781425034293_weights.h5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 235.0660 - val_loss: 238.2402\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 211.1386\n",
      "Epoch 00002: val_loss improved from 238.24022 to 237.69926, saving model to ./experiments/0.08926782252449214_0.011603194685843438__0.16941698304195646_0.32718781425034293_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 210.9307 - val_loss: 237.6993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 208.5228\n",
      "Epoch 00003: val_loss improved from 237.69926 to 237.52004, saving model to ./experiments/0.08926782252449214_0.011603194685843438__0.16941698304195646_0.32718781425034293_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 208.3120 - val_loss: 237.5200\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 207.5074\n",
      "Epoch 00004: val_loss did not improve from 237.52004\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 207.3036 - val_loss: 237.6180\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 206.7097\n",
      "Epoch 00005: val_loss improved from 237.52004 to 237.45141, saving model to ./experiments/0.08926782252449214_0.011603194685843438__0.16941698304195646_0.32718781425034293_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 206.5087 - val_loss: 237.4514\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 205.9126\n",
      "Epoch 00006: val_loss did not improve from 237.45141\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 205.6834 - val_loss: 237.7420\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 205.4658\n",
      "Epoch 00007: val_loss improved from 237.45141 to 237.40403, saving model to ./experiments/0.08926782252449214_0.011603194685843438__0.16941698304195646_0.32718781425034293_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 205.2798 - val_loss: 237.4040\n",
      "21/21 [==============================] - 5s 259ms/step\n",
      "** write log to ./experiments/0.08926782252449214_0.011603194685843438__0.16941698304195646_0.32718781425034293_test.log **\n",
      "mean auroc: 0.8603741219984389\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9401088790363802, 0.08926782252449214, 1.4195751593286225, 0.011603194685843438, 0.16941698304195646, 0.32718781425034293)\n",
      "Konten (0.9401088790363802, 0.08926782252449214, 1.4195751593286225, 0.011603194685843438, 0.16941698304195646, 0.32718781425034293)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=5,acq=\"weightedei\", n_iter=2, omega=0.1)\n",
    "time_took = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0:18:22.40\n",
      "{'target': 0.8608915704545345, 'params': {'alpha': 0.9340456763831478, 'dropout': 0.08425517073874295, 'gamma': 1.4195808780694898, 'lr': 0.016788704863628323, 'neuronPct': 0.1767547775828121, 'neuronShrink': 0.33180474398878285}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total runtime: {convert_string(time_took)}\")\n",
    "print(optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
