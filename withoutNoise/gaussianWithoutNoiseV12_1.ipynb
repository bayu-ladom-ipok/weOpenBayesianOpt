{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from callback import MultipleClassAUROC, MultiGPUModelCheckpoint\n",
    "from configparser import ConfigParser\n",
    "from generator import AugmentedImageSequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import multi_gpu_model\n",
    "from utility import get_sample_counts\n",
    "from weights import get_class_weights\n",
    "from augmenter import augmenter\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.keras.initializers\n",
    "import statistics\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer, Flatten, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_radam import RAdam\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "    # default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "image_source_dir = cp[\"DEFAULT\"].get(\"image_source_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "    # train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "positive_weights_multiply = cp[\"TRAIN\"].getfloat(\"positive_weights_multiply\")\n",
    "dataset_csv_dir = cp[\"TRAIN\"].get(\"dataset_csv_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=1.0, alpha=0.5):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network(dropout, neuronPct, neuronShrink):\n",
    "    # We start with some percent of 5000 starting neurons on the first hidden layer.\n",
    "    neuronCount = int(neuronPct * 5000)\n",
    "    # Construct neural network\n",
    "    neuronCount = neuronCount * neuronShrink\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,1536)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neuronCount))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(14, activation='sigmoid')) # Output\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** compute class weights from training data **\n",
      "** class_weights **\n",
      "[{0: 0.976060692178489, 1: 0.023939307821511}, {0: 0.9379028967906056, 1: 0.06209710320939444}, {0: 0.977507900874183, 1: 0.02249209912581691}, {0: 0.9801862148908839, 1: 0.01981378510911613}, {0: 0.9642020357560434, 1: 0.03579796424395662}, {0: 0.9663727015263743, 1: 0.033627298473625666}, {0: 0.8859702012473223, 1: 0.11402979875267771}, {0: 0.9586866934982315, 1: 0.04131330650176841}, {0: 0.9623146440112557, 1: 0.03768535598874437}, {0: 0.9298929992036218, 1: 0.07010700079637826}, {0: 0.9335352709009039, 1: 0.06646472909909606}, {0: 0.9021976306069932, 1: 0.09780236939300682}, {0: 0.9453965277787032, 1: 0.05460347222129675}, {1: 0.720226409263611, 0: 0.27977359073638897}]\n"
     ]
    }
   ],
   "source": [
    "# compute steps\n",
    "train_counts, train_pos_counts = get_sample_counts(output_dir, \"train\", class_names)\n",
    "dev_counts, _ = get_sample_counts(output_dir, \"dev\", class_names)\n",
    "    \n",
    "if train_steps == \"auto\":\n",
    "    train_steps = int(train_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        train_steps = int(train_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"train_steps: {train_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "if validation_steps == \"auto\":\n",
    "    validation_steps = int(dev_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        validation_steps = int(validation_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"validation_steps: {validation_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "        # compute class weights\n",
    "print(\"** compute class weights from training data **\")\n",
    "class_weights = get_class_weights(train_counts,train_pos_counts,multiply=positive_weights_multiply,)\n",
    "print(\"** class_weights **\")\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** test_steps: 21 **\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_steps = cp[\"TEST\"].get(\"test_steps\")\n",
    "test_counts, _ = get_sample_counts(output_dir, \"test\", class_names)\n",
    "\n",
    "if test_steps == \"auto\":\n",
    "    test_steps = int(test_counts / batch_size)\n",
    "else:\n",
    "    try:\n",
    "        test_steps = int(test_steps)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"\"\"test_steps: {test_steps} is invalid,please use 'auto' or integer.\"\"\")\n",
    "        \n",
    "print(f\"** test_steps: {test_steps} **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"train.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=train_steps,\n",
    "        )\n",
    "validation_sequence = AugmentedImageSequence(\n",
    "            dataset_csv_file=os.path.join(output_dir, \"dev.csv\"),\n",
    "            class_names=class_names,\n",
    "            source_image_dir=image_source_dir,\n",
    "            batch_size=batch_size,\n",
    "            target_size=(image_dimension, image_dimension),\n",
    "            augmenter=augmenter,\n",
    "            steps=validation_steps,\n",
    "            shuffle_on_epoch_end=False,\n",
    "        )\n",
    "\n",
    "test_sequence = AugmentedImageSequence(\n",
    "        dataset_csv_file=os.path.join(output_dir, \"test.csv\"),\n",
    "        class_names=class_names,\n",
    "        source_image_dir=image_source_dir,\n",
    "        batch_size=batch_size,\n",
    "        target_size=(image_dimension, image_dimension),\n",
    "        augmenter=None,\n",
    "        steps=test_steps,\n",
    "        shuffle_on_epoch_end=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_network(dropout,lr,neuronPct,neuronShrink,alpha,gamma):\n",
    "    # Define the Keras TensorBoard callback.\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir,  str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+output_weights_name)\n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "\n",
    "                 output_weights_path,\n",
    "                 save_weights_only=True,\n",
    "                 save_best_only=True,\n",
    "                 verbose=1,\n",
    "            )\n",
    "    start_time = time.time()\n",
    "    model = construct_network(dropout, neuronPct, neuronShrink)\n",
    "    \n",
    "    \n",
    "    #model.compile(loss=focal_loss(gamma=gamma,alpha=alpha), optimizer=SGD(lr=lr))\n",
    "    optimizer = SGD(lr=initial_learning_rate)\n",
    "    model.compile(optimizer=optimizer,loss=focal_loss(gamma=gamma,alpha=alpha))\n",
    "\n",
    "    #lookahead = Lookahead(k=5, alpha=0.5) # Initialize Lookahead\n",
    "    #lookahead.inject(model) # add into model\n",
    "    callbacks = [\n",
    "            checkpoint,\n",
    "            keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "            #TensorBoard(log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr), \n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto', restore_best_weights=True)\n",
    "        ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "            generator=train_sequence,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_sequence,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            workers=generator_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    y_hat = model.predict_generator(test_sequence, verbose=1)\n",
    "    y = test_sequence.get_y_true()\n",
    "    \n",
    "    test_log_path = os.path.join(output_dir, str(dropout)+\"_\"+str(lr)+\"_\"+\"_\"+str(neuronPct)+\"_\"+str(neuronShrink)+\"_\"+\"test.log\")\n",
    "    print(f\"** write log to {test_log_path} **\")\n",
    "    aurocs = []\n",
    "    \n",
    "    with open(test_log_path, \"w\") as f:\n",
    "        for i in range(len(class_names)):\n",
    "            try:\n",
    "                score = roc_auc_score(y[:, i], y_hat[:, i])\n",
    "                aurocs.append(score)\n",
    "            except ValueError:\n",
    "                score = 0\n",
    "            f.write(f\"{class_names[i]}: {score}\\n\")\n",
    "        mean_auroc = float(np.mean(aurocs))\n",
    "        f.write(\"-------------------------\\n\")\n",
    "        f.write(f\"mean auroc: {mean_auroc}\\n\")\n",
    "        print(f\"mean auroc: {mean_auroc}\")\n",
    "    \n",
    "\n",
    "    print(\"iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\")\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    time_took = time.time() - start_time\n",
    "    return mean_auroc\n",
    "\n",
    "        \n",
    "    \n",
    "    model.summary()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_bayes_opt.bayesian_optimization import BayesianOptimization\n",
    "import time\n",
    "\n",
    "# Supress NaN warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = { 'gamma':(1.3, 3.0),\n",
    "            'alpha':(0.5, 1.0),\n",
    "            'dropout': (0.07323, 0.07324),\n",
    "           'lr': (0.00923, 0.00924),\n",
    "           'neuronPct': (0.1944 , 0.1945),\n",
    "           'neuronShrink': (0.3521, 0.3522)\n",
    "          }\n",
    "\n",
    "#print(bounds.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kunci ['alpha', 'dropout', 'gamma', 'lr', 'neuronPct', 'neuronShrink']\n",
      "Bound  [[0.5     1.     ]\n",
      " [0.07323 0.07324]\n",
      " [1.3     3.     ]\n",
      " [0.00923 0.00924]\n",
      " [0.1944  0.1945 ]\n",
      " [0.3521  0.3522 ]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=optimize_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum \n",
    "    # is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "logger = JSONLogger(path=\"./without_noise12_1.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isine x [0.708511   0.0732372  1.30019444 0.00923302 0.19441468 0.35210923]\n",
      "Konten (0.7085110023512871, 0.07323720324493442, 1.3001944371894862, 0.009233023325726319, 0.1944146755890817, 0.3521092338594769)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.07323720324493442_0.009233023325726319__0.1944146755890817_0.3521092338594769_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-9-413a4d23effe>:44: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 445.9288\n",
      "Epoch 00001: val_loss improved from inf to 440.18417, saving model to ./experiments/0.07323720324493442_0.009233023325726319__0.1944146755890817_0.3521092338594769_weights.h5\n",
      "76/76 [==============================] - 54s 717ms/step - loss: 444.8994 - val_loss: 440.1842\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 398.5173\n",
      "Epoch 00002: val_loss improved from 440.18417 to 439.96647, saving model to ./experiments/0.07323720324493442_0.009233023325726319__0.1944146755890817_0.3521092338594769_weights.h5\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 398.0270 - val_loss: 439.9665\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 393.9588\n",
      "Epoch 00003: val_loss did not improve from 439.96647\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 393.4939 - val_loss: 440.4033\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 388.9281\n",
      "Epoch 00004: val_loss improved from 439.96647 to 437.81151, saving model to ./experiments/0.07323720324493442_0.009233023325726319__0.1944146755890817_0.3521092338594769_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 388.4889 - val_loss: 437.8115\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 388.7247\n",
      "Epoch 00005: val_loss did not improve from 437.81151\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 388.3031 - val_loss: 437.8142\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 388.5383\n",
      "Epoch 00006: val_loss improved from 437.81151 to 437.70249, saving model to ./experiments/0.07323720324493442_0.009233023325726319__0.1944146755890817_0.3521092338594769_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 388.0798 - val_loss: 437.7025\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 388.4319\n",
      "Epoch 00007: val_loss improved from 437.70249 to 437.68018, saving model to ./experiments/0.07323720324493442_0.009233023325726319__0.1944146755890817_0.3521092338594769_weights.h5\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 387.9707 - val_loss: 437.6802\n",
      "WARNING:tensorflow:From <ipython-input-9-413a4d23effe>:46: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.predict, which supports generators.\n",
      "21/21 [==============================] - 18s 859ms/step\n",
      "** write log to ./experiments/0.07323720324493442_0.009233023325726319__0.1944146755890817_0.3521092338594769_test.log **\n",
      "mean auroc: 0.8592532824199492\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.7085110023512871, 0.07323720324493442, 1.3001944371894862, 0.009233023325726319, 0.1944146755890817, 0.3521092338594769)\n",
      "Konten (0.7085110023512871, 0.07323720324493442, 1.3001944371894862, 0.009233023325726319, 0.1944146755890817, 0.3521092338594769)\n",
      "Isine x [0.59313011 0.07323346 1.97450471 0.00923539 0.19444192 0.35216852]\n",
      "Konten (0.5931301056888354, 0.07323345560727043, 1.974504706192139, 0.009235388167340033, 0.19444191945144032, 0.35216852195003967)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.07323345560727043_0.009235388167340033__0.19444191945144032_0.35216852195003967_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 299.7374\n",
      "Epoch 00001: val_loss improved from inf to 288.20962, saving model to ./experiments/0.07323345560727043_0.009235388167340033__0.19444191945144032_0.35216852195003967_weights.h5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 298.9956 - val_loss: 288.2096\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 262.8942\n",
      "Epoch 00002: val_loss did not improve from 288.20962\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 262.5827 - val_loss: 288.3216\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 258.3345\n",
      "Epoch 00003: val_loss improved from 288.20962 to 286.87191, saving model to ./experiments/0.07323345560727043_0.009235388167340033__0.19444191945144032_0.35216852195003967_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 258.0751 - val_loss: 286.8719\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 258.1617\n",
      "Epoch 00004: val_loss did not improve from 286.87191\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 257.8804 - val_loss: 286.8948\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 258.0001\n",
      "Epoch 00005: val_loss improved from 286.87191 to 286.83731, saving model to ./experiments/0.07323345560727043_0.009235388167340033__0.19444191945144032_0.35216852195003967_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 257.7165 - val_loss: 286.8373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 257.8157\n",
      "Epoch 00006: val_loss improved from 286.83731 to 286.82307, saving model to ./experiments/0.07323345560727043_0.009235388167340033__0.19444191945144032_0.35216852195003967_weights.h5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 257.5235 - val_loss: 286.8231\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 257.8077\n",
      "Epoch 00007: val_loss improved from 286.82307 to 286.81917, saving model to ./experiments/0.07323345560727043_0.009235388167340033__0.19444191945144032_0.35216852195003967_weights.h5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 257.5335 - val_loss: 286.8192\n",
      "21/21 [==============================] - 5s 260ms/step\n",
      "** write log to ./experiments/0.07323345560727043_0.009235388167340033__0.19444191945144032_0.35216852195003967_test.log **\n",
      "mean auroc: 0.8586829734932444\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.5931301056888354, 0.07323345560727043, 1.974504706192139, 0.009235388167340033, 0.19444191945144032, 0.35216852195003967)\n",
      "Konten (0.5931301056888354, 0.07323345560727043, 1.974504706192139, 0.009235388167340033, 0.19444191945144032, 0.35216852195003967)\n",
      "Isine x [0.60222612 0.07323878 1.34655891 0.0092367  0.19444173 0.35215587]\n",
      "Konten (0.6022261248657588, 0.07323878117436391, 1.3465589084364744, 0.009236704675101784, 0.19444173048023672, 0.35215586898284457)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 453.1686\n",
      "Epoch 00001: val_loss improved from inf to 431.96532, saving model to ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_weights.h5\n",
      "76/76 [==============================] - 23s 296ms/step - loss: 452.0426 - val_loss: 431.9653\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 395.5394\n",
      "Epoch 00002: val_loss improved from 431.96532 to 431.25944, saving model to ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_weights.h5\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 395.0607 - val_loss: 431.2594\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 390.3372\n",
      "Epoch 00003: val_loss did not improve from 431.25944\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 389.9130 - val_loss: 431.5994\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 385.4953\n",
      "Epoch 00004: val_loss improved from 431.25944 to 429.73922, saving model to ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 385.0813 - val_loss: 429.7392\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 385.3862\n",
      "Epoch 00005: val_loss improved from 429.73922 to 429.71123, saving model to ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 384.9577 - val_loss: 429.7112\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 385.2156\n",
      "Epoch 00006: val_loss improved from 429.71123 to 429.64688, saving model to ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 384.8063 - val_loss: 429.6469\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 385.1191\n",
      "Epoch 00007: val_loss improved from 429.64688 to 429.64525, saving model to ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 384.6877 - val_loss: 429.6452\n",
      "21/21 [==============================] - 5s 257ms/step\n",
      "** write log to ./experiments/0.07323878117436391_0.009236704675101784__0.19444173048023672_0.35215586898284457_test.log **\n",
      "mean auroc: 0.8594862805338171\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.6022261248657588, 0.07323878117436391, 1.3465589084364744, 0.009236704675101784, 0.19444173048023672, 0.35215586898284457)\n",
      "Konten (0.6022261248657588, 0.07323878117436391, 1.3465589084364744, 0.009236704675101784, 0.19444173048023672, 0.35215586898284457)\n",
      "Isine x [0.57019347 0.07323198 2.66126577 0.00923968 0.19443134 0.35216923]\n",
      "Konten (0.5701934692976169, 0.07323198101489085, 2.6612657667484125, 0.009239682615757194, 0.19443134241781593, 0.35216923226156693)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 190.5905\n",
      "Epoch 00001: val_loss improved from inf to 184.27077, saving model to ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_weights.h5\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 190.1411 - val_loss: 184.2708\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 167.8674\n",
      "Epoch 00002: val_loss improved from 184.27077 to 184.21889, saving model to ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 167.6739 - val_loss: 184.2189\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 166.0410\n",
      "Epoch 00003: val_loss did not improve from 184.21889\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 165.8572 - val_loss: 184.3444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 164.4850\n",
      "Epoch 00004: val_loss improved from 184.21889 to 183.26927, saving model to ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 164.2949 - val_loss: 183.2693\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 164.4062\n",
      "Epoch 00005: val_loss improved from 183.26927 to 183.26262, saving model to ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 164.2247 - val_loss: 183.2626\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 164.3877\n",
      "Epoch 00006: val_loss improved from 183.26262 to 183.23833, saving model to ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 164.2055 - val_loss: 183.2383\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 164.2420\n",
      "Epoch 00007: val_loss improved from 183.23833 to 183.23173, saving model to ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 164.0590 - val_loss: 183.2317\n",
      "21/21 [==============================] - 5s 258ms/step\n",
      "** write log to ./experiments/0.07323198101489085_0.009239682615757194__0.19443134241781593_0.35216923226156693_test.log **\n",
      "mean auroc: 0.8595786288722161\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.5701934692976169, 0.07323198101489085, 2.6612657667484125, 0.009239682615757194, 0.19443134241781593, 0.35216923226156693)\n",
      "Konten (0.5701934692976169, 0.07323198101489085, 2.6612657667484125, 0.009239682615757194, 0.19443134241781593, 0.35216923226156693)\n",
      "Isine x [0.93819458 0.07323895 1.44457516 0.00923039 0.19441698 0.35218781]\n",
      "Konten (0.9381945761480192, 0.07323894606663504, 1.4445751593286225, 0.00923039054783233, 0.19441698304195645, 0.35218781425034296)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.07323894606663504_0.00923039054783233__0.19441698304195645_0.35218781425034296_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 234.5091\n",
      "Epoch 00001: val_loss improved from inf to 238.56638, saving model to ./experiments/0.07323894606663504_0.00923039054783233__0.19441698304195645_0.35218781425034296_weights.h5\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 234.0377 - val_loss: 238.5664\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 210.1439\n",
      "Epoch 00002: val_loss improved from 238.56638 to 238.05379, saving model to ./experiments/0.07323894606663504_0.00923039054783233__0.19441698304195645_0.35218781425034296_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 209.9246 - val_loss: 238.0538\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 207.6619\n",
      "Epoch 00003: val_loss improved from 238.05379 to 237.65780, saving model to ./experiments/0.07323894606663504_0.00923039054783233__0.19441698304195645_0.35218781425034296_weights.h5\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 207.4561 - val_loss: 237.6578\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 206.3684\n",
      "Epoch 00004: val_loss did not improve from 237.65780\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 206.1698 - val_loss: 237.8162\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 204.7432\n",
      "Epoch 00005: val_loss improved from 237.65780 to 235.34039, saving model to ./experiments/0.07323894606663504_0.00923039054783233__0.19441698304195645_0.35218781425034296_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 204.5143 - val_loss: 235.3404\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 204.6375\n",
      "Epoch 00006: val_loss improved from 235.34039 to 235.30081, saving model to ./experiments/0.07323894606663504_0.00923039054783233__0.19441698304195645_0.35218781425034296_weights.h5\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 204.4337 - val_loss: 235.3008\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 204.4126\n",
      "Epoch 00007: val_loss did not improve from 235.30081\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 204.1958 - val_loss: 235.3588\n",
      "21/21 [==============================] - 5s 260ms/step\n",
      "** write log to ./experiments/0.07323894606663504_0.00923039054783233__0.19441698304195645_0.35218781425034296_test.log **\n",
      "mean auroc: 0.8602087028848369\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9381945761480192, 0.07323894606663504, 1.4445751593286225, 0.00923039054783233, 0.19441698304195645, 0.35218781425034296)\n",
      "Konten (0.9381945761480192, 0.07323894606663504, 1.4445751593286225, 0.00923039054783233, 0.19441698304195645, 0.35218781425034296)\n",
      "Isine x [0.94447975 0.07323865 1.44889071 0.00923    0.19441986 0.35218956]\n",
      "Konten (0.9444797518217164, 0.07323864575471516, 1.448890713827503, 0.00923, 0.1944198563064689, 0.3521895588218734)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.07323864575471516_0.00923__0.1944198563064689_0.3521895588218734_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 221.0209\n",
      "Epoch 00001: val_loss improved from inf to 225.78198, saving model to ./experiments/0.07323864575471516_0.00923__0.1944198563064689_0.3521895588218734_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 220.5896 - val_loss: 225.7820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 198.9957\n",
      "Epoch 00002: val_loss improved from 225.78198 to 225.68469, saving model to ./experiments/0.07323864575471516_0.00923__0.1944198563064689_0.3521895588218734_weights.h5\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 198.7904 - val_loss: 225.6847\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 196.7040\n",
      "Epoch 00003: val_loss did not improve from 225.68469\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 196.5144 - val_loss: 225.6992\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 194.9411\n",
      "Epoch 00004: val_loss improved from 225.68469 to 223.33409, saving model to ./experiments/0.07323864575471516_0.00923__0.1944198563064689_0.3521895588218734_weights.h5\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 194.7350 - val_loss: 223.3341\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 194.7140\n",
      "Epoch 00005: val_loss improved from 223.33409 to 223.32560, saving model to ./experiments/0.07323864575471516_0.00923__0.1944198563064689_0.3521895588218734_weights.h5\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 194.5235 - val_loss: 223.3256\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 194.5588\n",
      "Epoch 00006: val_loss did not improve from 223.32560\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 194.3732 - val_loss: 223.3325\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 194.4427\n",
      "Epoch 00007: val_loss improved from 223.32560 to 223.27727, saving model to ./experiments/0.07323864575471516_0.00923__0.1944198563064689_0.3521895588218734_weights.h5\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 194.2573 - val_loss: 223.2773\n",
      "21/21 [==============================] - 5s 256ms/step\n",
      "** write log to ./experiments/0.07323864575471516_0.00923__0.1944198563064689_0.3521895588218734_test.log **\n",
      "mean auroc: 0.8598035159509136\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.9444797518217164, 0.07323864575471516, 1.448890713827503, 0.00923, 0.1944198563064689, 0.3521895588218734)\n",
      "Konten (0.9444797518217164, 0.07323864575471516, 1.448890713827503, 0.00923, 0.1944198563064689, 0.3521895588218734)\n",
      "Isine x [0.70651592 0.07323369 1.30086943 0.00923834 0.19445626 0.35216794]\n",
      "Konten (0.7065159177626684, 0.07323368750657194, 1.300869428557757, 0.009238339734124617, 0.1944562640324099, 0.35216793540535646)\n",
      "Masuk\n",
      "** set output weights path to: ./experiments/0.07323368750657194_0.009238339734124617__0.1944562640324099_0.35216793540535646_weights.h5 **\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 342)               525654    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                4802      \n",
      "=================================================================\n",
      "Total params: 530,456\n",
      "Trainable params: 530,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 76 steps, validate for 10 steps\n",
      "Epoch 1/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 450.1740\n",
      "Epoch 00001: val_loss improved from inf to 440.18028, saving model to ./experiments/0.07323368750657194_0.009238339734124617__0.1944562640324099_0.35216793540535646_weights.h5\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 449.1093 - val_loss: 440.1803\n",
      "Epoch 2/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 399.4563\n",
      "Epoch 00002: val_loss did not improve from 440.18028\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 398.9455 - val_loss: 440.8141\n",
      "Epoch 3/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 391.6407\n",
      "Epoch 00003: val_loss improved from 440.18028 to 438.10147, saving model to ./experiments/0.07323368750657194_0.009238339734124617__0.1944562640324099_0.35216793540535646_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 391.1764 - val_loss: 438.1015\n",
      "Epoch 4/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 391.1795\n",
      "Epoch 00004: val_loss did not improve from 438.10147\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 390.7242 - val_loss: 438.1238\n",
      "Epoch 5/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 390.8895\n",
      "Epoch 00005: val_loss improved from 438.10147 to 437.97325, saving model to ./experiments/0.07323368750657194_0.009238339734124617__0.1944562640324099_0.35216793540535646_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 390.4469 - val_loss: 437.9733\n",
      "Epoch 6/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 390.7515\n",
      "Epoch 00006: val_loss improved from 437.97325 to 437.93673, saving model to ./experiments/0.07323368750657194_0.009238339734124617__0.1944562640324099_0.35216793540535646_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 390.3045 - val_loss: 437.9367\n",
      "Epoch 7/7\n",
      "75/76 [============================>.] - ETA: 0s - loss: 390.7081\n",
      "Epoch 00007: val_loss improved from 437.93673 to 437.91683, saving model to ./experiments/0.07323368750657194_0.009238339734124617__0.1944562640324099_0.35216793540535646_weights.h5\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 390.2537 - val_loss: 437.9168\n",
      "21/21 [==============================] - 5s 259ms/step\n",
      "** write log to ./experiments/0.07323368750657194_0.009238339734124617__0.1944562640324099_0.35216793540535646_test.log **\n",
      "mean auroc: 0.8596763419315289\n",
      "iteration|auroc|alpha|dropout|gamma|learning_rate|neuronPct|neuronShrink\n",
      "Konten (0.7065159177626684, 0.07323368750657194, 1.300869428557757, 0.009238339734124617, 0.1944562640324099, 0.35216793540535646)\n",
      "Konten (0.7065159177626684, 0.07323368750657194, 1.300869428557757, 0.009238339734124617, 0.1944562640324099, 0.35216793540535646)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=5,acq=\"weightedei\", n_iter=2, omega=0.9)\n",
    "time_took = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0:19:49.26\n",
      "{'target': 0.8602087028848369, 'params': {'alpha': 0.9381945761480192, 'dropout': 0.07323894606663504, 'gamma': 1.4445751593286225, 'lr': 0.00923039054783233, 'neuronPct': 0.19441698304195645, 'neuronShrink': 0.35218781425034296}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total runtime: {convert_string(time_took)}\")\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4869"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "params\n"
     ]
    }
   ],
   "source": [
    "new_params = {} \n",
    "for t in optimizer.max.keys():\n",
    "    print(t)\n",
    "    new_params = optimizer.max.get(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = new_params.get('alpha')\n",
    "d = new_params.get('dropout')\n",
    "g = new_params.get('gamma')\n",
    "l = new_params.get('lr')\n",
    "np = new_params.get('neuronPct')\n",
    "ns = new_params.get('neuronShrink')\n",
    "new_params.update({'alpha':(float(a-0.025),float(a+0.025)),\n",
    "                   'dropout':(float(d-0.025),float(d+0.025)),\n",
    "                   'gamma':(float(g-0.025),float(g+0.025)),\n",
    "                   'lr':(float(l),float(l+0.025)),                   \n",
    "                   'neuronPct':(float(np-0.025),float(np+0.025)),\n",
    "                   'neuronShrink':(float(ns-0.025),float(ns+0.025))\n",
    "                  }\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': (0.9131945761480191, 0.9631945761480192),\n",
       " 'dropout': (0.04823894606663504, 0.09823894606663505),\n",
       " 'gamma': (1.4195751593286225, 1.4695751593286224),\n",
       " 'lr': (0.00923039054783233, 0.034230390547832334),\n",
       " 'neuronPct': (0.16941698304195646, 0.21941698304195645),\n",
       " 'neuronShrink': (0.32718781425034293, 0.377187814250343)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "w = csv.writer(open(\"new_params.csv\", \"w\"))\n",
    "for key, val in new_params.items():\n",
    "    w.writerow([key, val])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
